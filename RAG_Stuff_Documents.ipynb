{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda \n",
    "from langchain.chains import LLMChain, StuffDocumentsChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"\n",
    "            Use the following portion of a long document to answer the question. Answer based on the given context.\n",
    "            If there is no relevant text and you cannot answer, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=map_doc_prompt)\n",
    "\n",
    "map_doc_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,  \n",
    "    memory=memory,\n",
    "    document_variable_name=\"context\"  \n",
    ")\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"extra\": RunnablePassthrough(),\n",
    "    }\n",
    "    | final_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, according to the document, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Is Aaronson guilty?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In the document, the message he wrote on the table was \"2+2=5.\"')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What message did he write in the table?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Julia is a character in the document who is associated with the protagonist.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is Julia?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
